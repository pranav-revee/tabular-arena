{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¦ Tabular Arena â€” Home Credit Default Risk Benchmark\n",
    "\n",
    "**Self-contained** â€” downloads data from Kaggle, runs all 9 models, saves results.\n",
    "\n",
    "**Runtime**: Use **GPU** (A100/H100) â†’ Runtime â†’ Change runtime type â†’ GPU\n",
    "\n",
    "| Step | Cell | Time |\n",
    "|------|------|------|\n",
    "| 1 | Install deps + verify GPU | ~2 min |\n",
    "| 2 | Download dataset from Kaggle | ~1 min |\n",
    "| 3 | Feature engineering | ~3 min |\n",
    "| 4 | Run all 9 models | ~45-90 min |\n",
    "| 5 | Download results JSON | instant |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 1: Install dependencies + verify GPU\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "!pip install -q lightgbm xgboost catboost autogluon.tabular optuna tabpfn psutil\n",
    "\n",
    "import torch, platform, psutil\n",
    "print(f'Python: {platform.python_version()}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    total = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)} ({total / 1024**3:.0f}GB)')\n",
    "print(f'RAM: {psutil.virtual_memory().total / 1024**3:.0f}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 2: Download dataset from Kaggle\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Set your KAGGLE_API_TOKEN in Colab Secrets (ğŸ”‘ sidebar):\n",
    "#   Name:  KAGGLE_API_TOKEN\n",
    "#   Value: your token from kaggle.com/settings\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import userdata\n",
    "KAGGLE_TOKEN = userdata.get('KAGGLE_API_TOKEN')\n",
    "print('âœ… Kaggle API token loaded')\n",
    "\n",
    "DATA_DIR = Path('/content/data/home-credit-default-risk')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "required_files = [\n",
    "    'application_train.csv', 'bureau.csv', 'bureau_balance.csv',\n",
    "    'previous_application.csv', 'installments_payments.csv',\n",
    "    'POS_CASH_balance.csv', 'credit_card_balance.csv',\n",
    "]\n",
    "\n",
    "missing = [f for f in required_files if not (DATA_DIR / f).exists()]\n",
    "\n",
    "if missing:\n",
    "    print('Downloading dataset from Kaggle API...')\n",
    "    import subprocess\n",
    "    subprocess.run([\n",
    "        'curl', '-L', '-o', '/content/data/home-credit-default-risk.zip',\n",
    "        '-H', f'Authorization: Bearer {KAGGLE_TOKEN}',\n",
    "        'https://www.kaggle.com/api/v1/competitions/data/download/home-credit-default-risk'\n",
    "    ], check=True)\n",
    "    !cd /content/data && unzip -o -q home-credit-default-risk.zip -d home-credit-default-risk\n",
    "    !rm -f /content/data/home-credit-default-risk.zip\n",
    "\n",
    "print('\\nDataset files:')\n",
    "for f in required_files:\n",
    "    path = DATA_DIR / f\n",
    "    sz = path.stat().st_size / (1024*1024) if path.exists() else 0\n",
    "    status = 'âœ…' if path.exists() else 'âŒ MISSING'\n",
    "    print(f'  {f:<35s} {sz:>6.0f}MB  {status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 3: Configuration + feature engineering\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import platform\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATA_DIR = Path('/content/data/home-credit-default-risk')\n",
    "OUTPUT_PATH = Path('/content/credit_results.json')\n",
    "\n",
    "SCALING_SIZES = [1000, 5000, 20000, 50000, 150000, 245000]\n",
    "TABPFN_MAX_SAMPLES = 30000       # 80GB VRAM â€” 3x more data for TabPFN\n",
    "RANDOM_STATE = 42\n",
    "OPTUNA_TRIALS = 30               # sweet spot: enough trials, GPU makes them fast\n",
    "CV_FOLDS = 5\n",
    "\n",
    "import torch\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = 'cuda' if USE_GPU else 'cpu'\n",
    "print(f'Device: {DEVICE.upper()} | GPU: {USE_GPU}')\n",
    "if USE_GPU:\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# â”€â”€ Hardware info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_hardware_info():\n",
    "    cpu = platform.processor() or 'unknown'\n",
    "    try:\n",
    "        with open('/proc/cpuinfo') as f:\n",
    "            for line in f:\n",
    "                if 'model name' in line:\n",
    "                    cpu = line.split(':')[1].strip()\n",
    "                    break\n",
    "    except Exception:\n",
    "        pass\n",
    "    ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
    "    info = {\n",
    "        'cpu': cpu,\n",
    "        'ram_gb': round(ram_gb, 1),\n",
    "        'os': f'{platform.system()} {platform.release()}',\n",
    "        'python': platform.python_version(),\n",
    "    }\n",
    "    if USE_GPU:\n",
    "        info['gpu'] = torch.cuda.get_device_name(0)\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        total = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n",
    "        info['gpu_memory_gb'] = round(total / 1024**3, 1)\n",
    "    return info\n",
    "\n",
    "# â”€â”€ Multi-table feature engineering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _agg_numeric(df, group_col, prefix):\n",
    "    num_cols = df.select_dtypes(include='number').columns.drop(group_col, errors='ignore')\n",
    "    agg = df.groupby(group_col)[num_cols].agg(['mean', 'max', 'min', 'sum']).reset_index()\n",
    "    agg.columns = [group_col] + [f'{prefix}_{c[0]}_{c[1]}' for c in agg.columns[1:]]\n",
    "    counts = df.groupby(group_col).size().reset_index(name=f'{prefix}_count')\n",
    "    return agg.merge(counts, on=group_col, how='left')\n",
    "\n",
    "\n",
    "def build_features():\n",
    "    print('\\n  Loading all dataset tables...')\n",
    "    t0 = time.time()\n",
    "\n",
    "    app = pd.read_csv(DATA_DIR / 'application_train.csv')\n",
    "    bureau = pd.read_csv(DATA_DIR / 'bureau.csv')\n",
    "    bb = pd.read_csv(DATA_DIR / 'bureau_balance.csv')\n",
    "    prev = pd.read_csv(DATA_DIR / 'previous_application.csv')\n",
    "    ins = pd.read_csv(DATA_DIR / 'installments_payments.csv')\n",
    "    pos = pd.read_csv(DATA_DIR / 'POS_CASH_balance.csv')\n",
    "    cc = pd.read_csv(DATA_DIR / 'credit_card_balance.csv')\n",
    "\n",
    "    print(f'    Loaded 7 tables in {time.time() - t0:.1f}s')\n",
    "\n",
    "    # Application table cleanup\n",
    "    app = app.drop(columns=['SK_ID_CURR'], errors='ignore')\n",
    "    app['DAYS_EMPLOYED'] = app['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "    app_raw = pd.read_csv(DATA_DIR / 'application_train.csv', usecols=['SK_ID_CURR'])\n",
    "    app.insert(0, 'SK_ID_CURR', app_raw['SK_ID_CURR'])\n",
    "\n",
    "    # Bureau features\n",
    "    print('    Engineering bureau features...')\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(\n",
    "        bb_months_count=('MONTHS_BALANCE', 'count'),\n",
    "        bb_months_min=('MONTHS_BALANCE', 'min'),\n",
    "        bb_dpd_status_sum=('STATUS', lambda x: (x.astype(str).str.isdigit().astype(int) * x.astype(str).apply(lambda v: int(v) if v.isdigit() else 0)).sum()),\n",
    "    ).reset_index()\n",
    "\n",
    "    bureau = bureau.merge(bb_agg, on='SK_ID_BUREAU', how='left')\n",
    "    bureau_agg = _agg_numeric(bureau, 'SK_ID_CURR', 'bur')\n",
    "\n",
    "    bureau['CREDIT_ACTIVE_BIN'] = (bureau['CREDIT_ACTIVE'] == 'Active').astype(int)\n",
    "    bureau['CREDIT_CLOSED_BIN'] = (bureau['CREDIT_ACTIVE'] == 'Closed').astype(int)\n",
    "    bur_status = bureau.groupby('SK_ID_CURR').agg(\n",
    "        bur_active_count=('CREDIT_ACTIVE_BIN', 'sum'),\n",
    "        bur_closed_count=('CREDIT_CLOSED_BIN', 'sum'),\n",
    "        bur_credit_day_overdue_max=('CREDIT_DAY_OVERDUE', 'max'),\n",
    "        bur_debt_ratio=('AMT_CREDIT_SUM_DEBT', lambda x: x.sum() / max(x.count(), 1)),\n",
    "    ).reset_index()\n",
    "    bureau_agg = bureau_agg.merge(bur_status, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Previous application features\n",
    "    print('    Engineering previous application features...')\n",
    "    prev_agg = _agg_numeric(prev, 'SK_ID_CURR', 'prev')\n",
    "    prev['APPROVED'] = (prev['NAME_CONTRACT_STATUS'] == 'Approved').astype(int)\n",
    "    prev['REFUSED'] = (prev['NAME_CONTRACT_STATUS'] == 'Refused').astype(int)\n",
    "    prev_status = prev.groupby('SK_ID_CURR').agg(\n",
    "        prev_approved_count=('APPROVED', 'sum'),\n",
    "        prev_refused_count=('REFUSED', 'sum'),\n",
    "        prev_approval_rate=('APPROVED', 'mean'),\n",
    "    ).reset_index()\n",
    "    prev_agg = prev_agg.merge(prev_status, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Installments features\n",
    "    print('    Engineering installment features...')\n",
    "    ins['PAYMENT_DELAY'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['PAYMENT_SHORTFALL'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    ins['PAYMENT_RATIO'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT'].replace(0, np.nan)\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(\n",
    "        ins_count=('SK_ID_PREV', 'count'),\n",
    "        ins_delay_mean=('PAYMENT_DELAY', 'mean'),\n",
    "        ins_delay_max=('PAYMENT_DELAY', 'max'),\n",
    "        ins_delay_positive_count=('PAYMENT_DELAY', lambda x: (x > 0).sum()),\n",
    "        ins_shortfall_mean=('PAYMENT_SHORTFALL', 'mean'),\n",
    "        ins_shortfall_max=('PAYMENT_SHORTFALL', 'max'),\n",
    "        ins_payment_ratio_mean=('PAYMENT_RATIO', 'mean'),\n",
    "        ins_payment_ratio_min=('PAYMENT_RATIO', 'min'),\n",
    "        ins_amt_payment_sum=('AMT_PAYMENT', 'sum'),\n",
    "        ins_amt_instalment_sum=('AMT_INSTALMENT', 'sum'),\n",
    "    ).reset_index()\n",
    "\n",
    "    # POS Cash balance features\n",
    "    print('    Engineering POS cash features...')\n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(\n",
    "        pos_count=('SK_ID_PREV', 'count'),\n",
    "        pos_dpd_max=('SK_DPD', 'max'),\n",
    "        pos_dpd_mean=('SK_DPD', 'mean'),\n",
    "        pos_dpd_def_max=('SK_DPD_DEF', 'max'),\n",
    "        pos_months_balance_min=('MONTHS_BALANCE', 'min'),\n",
    "        pos_months_balance_max=('MONTHS_BALANCE', 'max'),\n",
    "        pos_instalment_future_mean=('CNT_INSTALMENT_FUTURE', 'mean'),\n",
    "    ).reset_index()\n",
    "    pos['IS_LATE'] = (pos['SK_DPD'] > 0).astype(int)\n",
    "    pos_late = pos.groupby('SK_ID_CURR').agg(pos_late_ratio=('IS_LATE', 'mean')).reset_index()\n",
    "    pos_agg = pos_agg.merge(pos_late, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Credit card features\n",
    "    print('    Engineering credit card features...')\n",
    "    cc['CC_UTILIZATION'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL'].replace(0, np.nan)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(\n",
    "        cc_count=('SK_ID_PREV', 'count'),\n",
    "        cc_balance_mean=('AMT_BALANCE', 'mean'),\n",
    "        cc_balance_max=('AMT_BALANCE', 'max'),\n",
    "        cc_credit_limit_mean=('AMT_CREDIT_LIMIT_ACTUAL', 'mean'),\n",
    "        cc_utilization_mean=('CC_UTILIZATION', 'mean'),\n",
    "        cc_utilization_max=('CC_UTILIZATION', 'max'),\n",
    "        cc_dpd_max=('SK_DPD', 'max'),\n",
    "        cc_dpd_mean=('SK_DPD', 'mean'),\n",
    "        cc_drawings_atm_mean=('AMT_DRAWINGS_ATM_CURRENT', 'mean'),\n",
    "        cc_payment_current_mean=('AMT_PAYMENT_CURRENT', 'mean'),\n",
    "        cc_months_balance_min=('MONTHS_BALANCE', 'min'),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge everything\n",
    "    print('    Merging all features...')\n",
    "    df = app.copy()\n",
    "    for agg_df in [bureau_agg, prev_agg, ins_agg, pos_agg, cc_agg]:\n",
    "        df = df.merge(agg_df, on='SK_ID_CURR', how='left')\n",
    "    df = df.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "    n_original = app.shape[1] - 2\n",
    "    n_engineered = df.shape[1] - 1 - n_original\n",
    "    print(f'    Original features: {n_original}')\n",
    "    print(f'    Engineered features: {n_engineered}')\n",
    "    print(f'    Total features: {df.shape[1] - 1}')\n",
    "    print(f'    Feature engineering done in {time.time() - t0:.1f}s\\n')\n",
    "\n",
    "    return df, df['TARGET'].mean()\n",
    "\n",
    "\n",
    "# â”€â”€ Build features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df, target_rate = build_features()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=['TARGET']), df['TARGET'],\n",
    "    test_size=0.2, stratify=df['TARGET'], random_state=RANDOM_STATE\n",
    ")\n",
    "X_full = df.drop(columns=['TARGET'])\n",
    "y_full = df['TARGET']\n",
    "\n",
    "print(f'  Train: {len(X_train)} rows | Test: {len(X_test)} rows')\n",
    "print(f'  Features: {X_train.shape[1]} | Target rate: {target_rate:.3f}')\n",
    "\n",
    "# â”€â”€ Results container â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results = {\n",
    "    'dataset': 'home_credit',\n",
    "    'n_samples': len(df),\n",
    "    'n_features': df.shape[1] - 1,\n",
    "    'target_rate': round(target_rate, 3),\n",
    "    'hardware': get_hardware_info(),\n",
    "    'timestamp': datetime.now(timezone.utc).isoformat(),\n",
    "    'models': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 4: Helper functions\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def encode_for_lgbm(X_train, X_test):\n",
    "    X_tr = X_train.copy()\n",
    "    X_te = X_test.copy()\n",
    "    encoders = {}\n",
    "    for col in X_tr.select_dtypes(include='object').columns:\n",
    "        le = LabelEncoder()\n",
    "        combined = pd.concat([X_tr[col], X_te[col]]).astype(str)\n",
    "        le.fit(combined)\n",
    "        X_tr[col] = le.transform(X_tr[col].astype(str))\n",
    "        X_te[col] = le.transform(X_te[col].astype(str))\n",
    "        encoders[col] = le\n",
    "    return X_tr, X_te, encoders\n",
    "\n",
    "\n",
    "def encode_for_numeric(X_train, X_test):\n",
    "    X_tr, X_te, _ = encode_for_lgbm(X_train, X_test)\n",
    "    X_tr = X_tr.fillna(X_tr.median())\n",
    "    X_te = X_te.fillna(X_tr.median())\n",
    "    return X_tr, X_te\n",
    "\n",
    "\n",
    "def measure_model(name, train_fn, predict_fn, X_train, X_test, y_train, y_test):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f'  Running: {name}')\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    rss_before = process.memory_info().rss\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = train_fn(X_train, y_train)\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    rss_after = process.memory_info().rss\n",
    "    peak_memory_mb = max(0.0, (rss_after - rss_before) / (1024 * 1024))\n",
    "\n",
    "    inf_start = time.time()\n",
    "    y_prob = predict_fn(model, X_test)\n",
    "    inf_time = time.time() - inf_start\n",
    "    inf_per_1k = (inf_time / len(X_test)) * 1000 * 1000\n",
    "\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    ll = log_loss(y_test, y_prob)\n",
    "\n",
    "    print(f'  AUC: {auc:.4f} | Log Loss: {ll:.4f}')\n",
    "    print(f'  Train: {train_time:.1f}s | Memory: {peak_memory_mb:.0f}MB | Inference: {inf_per_1k:.1f}ms/1k')\n",
    "\n",
    "    return model, {\n",
    "        'auc_roc': round(auc, 4),\n",
    "        'log_loss': round(ll, 4),\n",
    "        'train_time_sec': round(train_time, 2),\n",
    "        'inference_time_ms_per_1k': round(inf_per_1k, 1),\n",
    "        'peak_memory_mb': round(peak_memory_mb, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "def cv_auc(train_fn, predict_fn, X, y, prep_fn=None):\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    aucs = []\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "        if prep_fn:\n",
    "            X_tr, X_val = prep_fn(X_tr, X_val)\n",
    "        model = train_fn(X_tr, y_tr)\n",
    "        y_prob = predict_fn(model, X_val)\n",
    "        aucs.append(roc_auc_score(y_val, y_prob))\n",
    "        print(f'    Fold {fold+1}: {aucs[-1]:.4f}')\n",
    "    return round(np.mean(aucs), 4)\n",
    "\n",
    "\n",
    "def scaling_curve(train_fn, predict_fn, X_train, X_test, y_train, y_test, prep_fn=None):\n",
    "    points = []\n",
    "    for n in SCALING_SIZES:\n",
    "        if n > len(X_train):\n",
    "            break\n",
    "        idx = X_train.sample(n=n, random_state=RANDOM_STATE).index\n",
    "        X_sub, y_sub = X_train.loc[idx], y_train.loc[idx]\n",
    "        X_te_local, X_sub_local = X_test.copy(), X_sub.copy()\n",
    "        if prep_fn:\n",
    "            X_sub_local, X_te_local = prep_fn(X_sub_local, X_te_local)\n",
    "        model = train_fn(X_sub_local, y_sub)\n",
    "        y_prob = predict_fn(model, X_te_local)\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        points.append({'n_samples': n, 'auc': round(auc, 4)})\n",
    "        print(f'    n={n}: AUC={auc:.4f}')\n",
    "    return points\n",
    "\n",
    "\n",
    "def _lgbm_params():\n",
    "    base = {'objective': 'binary', 'metric': 'auc', 'verbosity': -1}\n",
    "    if USE_GPU:\n",
    "        base['device'] = 'gpu'\n",
    "        base['gpu_use_dp'] = False\n",
    "        base['num_threads'] = os.cpu_count()\n",
    "        base['max_bin'] = 255              # LightGBM GPU max is 255\n",
    "    return base\n",
    "\n",
    "\n",
    "def _xgb_params():\n",
    "    base = {'objective': 'binary:logistic', 'eval_metric': 'auc', 'verbosity': 0}\n",
    "    if USE_GPU:\n",
    "        base['device'] = 'cuda'\n",
    "        base['tree_method'] = 'hist'\n",
    "        base['max_bin'] = 512              # finer splits with 80GB headroom\n",
    "    base['nthread'] = -1\n",
    "    return base\n",
    "\n",
    "\n",
    "def _catboost_kwargs():\n",
    "    kw = {'verbose': 0, 'random_seed': RANDOM_STATE, 'eval_metric': 'AUC'}\n",
    "    if USE_GPU:\n",
    "        kw['task_type'] = 'GPU'\n",
    "        kw['devices'] = '0'\n",
    "        kw['gpu_ram_part'] = 0.95           # use 95% of 80GB VRAM\n",
    "        kw['border_count'] = 254            # max histogram bins for GPU\n",
    "    return kw\n",
    "\n",
    "\n",
    "def _catboost_fill_nan(df, cat_cols):\n",
    "    df = df.copy()\n",
    "    for col in cat_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('missing').astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_progress():\n",
    "    with open(OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f'  >> Saved ({len(results[\"models\"])} models so far)')\n",
    "\n",
    "\n",
    "print('Helper functions ready âœ…')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 5: LightGBM (Default + Tuned)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# â”€â”€ LightGBM Default â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_lgbm_default():\n",
    "    def prep(Xtr, Xte):\n",
    "        return encode_for_lgbm(Xtr, Xte)[:2]\n",
    "\n",
    "    params = _lgbm_params()\n",
    "    def train_fn(X, y):\n",
    "        # High max rounds + early stopping = fast AND optimal\n",
    "        dtrain = lgb.Dataset(X, label=y)\n",
    "        # Split 10% for early stopping validation\n",
    "        from sklearn.model_selection import train_test_split as _tts\n",
    "        X_t, X_v, y_t, y_v = _tts(X, y, test_size=0.1, stratify=y, random_state=RANDOM_STATE)\n",
    "        dt = lgb.Dataset(X_t, label=y_t)\n",
    "        dv = lgb.Dataset(X_v, label=y_v, reference=dt)\n",
    "        return lgb.train(params, dt, num_boost_round=1000,\n",
    "                         valid_sets=[dv], callbacks=[lgb.early_stopping(30, verbose=False)])\n",
    "    def predict_fn(model, X):\n",
    "        return model.predict(X)\n",
    "\n",
    "    X_tr_enc, X_te_enc, _ = encode_for_lgbm(X_train, X_test)\n",
    "    _, metrics = measure_model('LightGBM (Default)', train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "\n",
    "    print('  CV AUC...')\n",
    "    metrics['auc_roc'] = cv_auc(train_fn, predict_fn, X_full, y_full, prep_fn=prep)\n",
    "    print(f'  CV AUC: {metrics[\"auc_roc\"]}')\n",
    "\n",
    "    print('  Scaling curve...')\n",
    "    scaling = scaling_curve(train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "\n",
    "    return {\n",
    "        'name': 'LightGBM (Default)', 'category': 'gradient_boosting', 'tuned': False,\n",
    "        'metrics': metrics, 'scaling': scaling,\n",
    "        'raw_data_handling': {'missing_values': 'native', 'categorical_features': 'needs_encoding', 'class_imbalance': 'scale_pos_weight'},\n",
    "    }\n",
    "\n",
    "# â”€â”€ LightGBM Tuned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_lgbm_tuned():\n",
    "    X_tr_enc, X_te_enc, _ = encode_for_lgbm(X_train, X_test)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = _lgbm_params()\n",
    "        params.update({\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        })\n",
    "        n_rounds = trial.suggest_int('n_rounds', 50, 500)\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "        aucs = []\n",
    "        for tr_idx, val_idx in skf.split(X_tr_enc, y_train):\n",
    "            dtrain = lgb.Dataset(X_tr_enc.iloc[tr_idx], label=y_train.iloc[tr_idx])\n",
    "            dval = lgb.Dataset(X_tr_enc.iloc[val_idx], label=y_train.iloc[val_idx])\n",
    "            model = lgb.train(params, dtrain, num_boost_round=n_rounds,\n",
    "                              valid_sets=[dval], callbacks=[lgb.early_stopping(10, verbose=False)])\n",
    "            pred = model.predict(X_tr_enc.iloc[val_idx])\n",
    "            aucs.append(roc_auc_score(y_train.iloc[val_idx], pred))\n",
    "        return np.mean(aucs)\n",
    "\n",
    "    search_start = time.time()\n",
    "    print(f'\\n  Optuna: {OPTUNA_TRIALS} trials...')\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=OPTUNA_TRIALS)\n",
    "    search_time = time.time() - search_start\n",
    "    best = study.best_params\n",
    "    n_rounds = best.pop('n_rounds')\n",
    "    best.update(_lgbm_params())\n",
    "\n",
    "    def train_fn(X, y):\n",
    "        return lgb.train(best, lgb.Dataset(X, label=y), num_boost_round=n_rounds)\n",
    "    def predict_fn(model, X):\n",
    "        return model.predict(X)\n",
    "\n",
    "    _, metrics = measure_model('LightGBM (Tuned)', train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "    metrics['search_time_sec'] = round(search_time, 2)\n",
    "    metrics['fit_time_sec'] = metrics['train_time_sec']\n",
    "    metrics['train_time_sec'] = round(search_time + metrics['fit_time_sec'], 2)\n",
    "\n",
    "    def prep(Xtr, Xte):\n",
    "        return encode_for_lgbm(Xtr, Xte)[:2]\n",
    "\n",
    "    print('  CV AUC...')\n",
    "    metrics['auc_roc'] = cv_auc(train_fn, predict_fn, X_full, y_full, prep_fn=prep)\n",
    "    print(f'  CV AUC: {metrics[\"auc_roc\"]}')\n",
    "\n",
    "    print('  Scaling curve...')\n",
    "    scaling = scaling_curve(train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "\n",
    "    return {\n",
    "        'name': 'LightGBM (Tuned)', 'category': 'gradient_boosting', 'tuned': True,\n",
    "        'metrics': metrics, 'scaling': scaling,\n",
    "        'raw_data_handling': {'missing_values': 'native', 'categorical_features': 'needs_encoding', 'class_imbalance': 'scale_pos_weight'},\n",
    "    }\n",
    "\n",
    "# â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for name, runner in [('LightGBM (Default)', run_lgbm_default), ('LightGBM (Tuned)', run_lgbm_tuned)]:\n",
    "    try:\n",
    "        results['models'].append(runner())\n",
    "        save_progress()\n",
    "    except Exception as e:\n",
    "        print(f'\\n  FAILED: {name} â€” {e}')\n",
    "        import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 6: XGBoost (Default + Tuned)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# â”€â”€ XGBoost Default â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_xgb_default():\n",
    "    params = _xgb_params()\n",
    "    def prep(Xtr, Xte):\n",
    "        return encode_for_lgbm(Xtr, Xte)[:2]\n",
    "    def train_fn(X, y):\n",
    "        from sklearn.model_selection import train_test_split as _tts\n",
    "        X_t, X_v, y_t, y_v = _tts(X, y, test_size=0.1, stratify=y, random_state=RANDOM_STATE)\n",
    "        dtrain = xgb.DMatrix(X_t, label=y_t)\n",
    "        dval = xgb.DMatrix(X_v, label=y_v)\n",
    "        return xgb.train(params, dtrain, num_boost_round=1000,\n",
    "                         evals=[(dval, 'val')], early_stopping_rounds=30, verbose_eval=False)\n",
    "    def predict_fn(model, X):\n",
    "        return model.predict(xgb.DMatrix(X))\n",
    "\n",
    "    X_tr_enc, X_te_enc, _ = encode_for_lgbm(X_train, X_test)\n",
    "    _, metrics = measure_model('XGBoost (Default)', train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "\n",
    "    print('  CV AUC...')\n",
    "    metrics['auc_roc'] = cv_auc(train_fn, predict_fn, X_full, y_full, prep_fn=prep)\n",
    "    print(f'  CV AUC: {metrics[\"auc_roc\"]}')\n",
    "\n",
    "    print('  Scaling curve...')\n",
    "    scaling = scaling_curve(train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "\n",
    "    return {\n",
    "        'name': 'XGBoost (Default)', 'category': 'gradient_boosting', 'tuned': False,\n",
    "        'metrics': metrics, 'scaling': scaling,\n",
    "        'raw_data_handling': {'missing_values': 'native', 'categorical_features': 'needs_encoding', 'class_imbalance': 'scale_pos_weight'},\n",
    "    }\n",
    "\n",
    "# â”€â”€ XGBoost Tuned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_xgb_tuned():\n",
    "    X_tr_enc, X_te_enc, _ = encode_for_lgbm(X_train, X_test)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = _xgb_params()\n",
    "        params.update({\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        })\n",
    "        n_rounds = trial.suggest_int('n_rounds', 50, 500)\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "        aucs = []\n",
    "        for tr_idx, val_idx in skf.split(X_tr_enc, y_train):\n",
    "            dtrain = xgb.DMatrix(X_tr_enc.iloc[tr_idx], label=y_train.iloc[tr_idx])\n",
    "            dval = xgb.DMatrix(X_tr_enc.iloc[val_idx], label=y_train.iloc[val_idx])\n",
    "            model = xgb.train(params, dtrain, num_boost_round=n_rounds,\n",
    "                              evals=[(dval, 'val')], early_stopping_rounds=10, verbose_eval=False)\n",
    "            pred = model.predict(dval)\n",
    "            aucs.append(roc_auc_score(y_train.iloc[val_idx], pred))\n",
    "        return np.mean(aucs)\n",
    "\n",
    "    search_start = time.time()\n",
    "    print(f'\\n  Optuna: {OPTUNA_TRIALS} trials...')\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=OPTUNA_TRIALS)\n",
    "    search_time = time.time() - search_start\n",
    "    best = study.best_params\n",
    "    n_rounds = best.pop('n_rounds')\n",
    "    best.update(_xgb_params())\n",
    "\n",
    "    def train_fn(X, y):\n",
    "        return xgb.train(best, xgb.DMatrix(X, label=y), num_boost_round=n_rounds)\n",
    "    def predict_fn(model, X):\n",
    "        return model.predict(xgb.DMatrix(X))\n",
    "\n",
    "    _, metrics = measure_model('XGBoost (Tuned)', train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "    metrics['search_time_sec'] = round(search_time, 2)\n",
    "    metrics['fit_time_sec'] = metrics['train_time_sec']\n",
    "    metrics['train_time_sec'] = round(search_time + metrics['fit_time_sec'], 2)\n",
    "\n",
    "    def prep(Xtr, Xte):\n",
    "        return encode_for_lgbm(Xtr, Xte)[:2]\n",
    "\n",
    "    print('  CV AUC...')\n",
    "    metrics['auc_roc'] = cv_auc(train_fn, predict_fn, X_full, y_full, prep_fn=prep)\n",
    "    print(f'  CV AUC: {metrics[\"auc_roc\"]}')\n",
    "\n",
    "    print('  Scaling curve...')\n",
    "    scaling = scaling_curve(train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "\n",
    "    return {\n",
    "        'name': 'XGBoost (Tuned)', 'category': 'gradient_boosting', 'tuned': True,\n",
    "        'metrics': metrics, 'scaling': scaling,\n",
    "        'raw_data_handling': {'missing_values': 'native', 'categorical_features': 'needs_encoding', 'class_imbalance': 'scale_pos_weight'},\n",
    "    }\n",
    "\n",
    "# â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for name, runner in [('XGBoost (Default)', run_xgb_default), ('XGBoost (Tuned)', run_xgb_tuned)]:\n",
    "    try:\n",
    "        results['models'].append(runner())\n",
    "        save_progress()\n",
    "    except Exception as e:\n",
    "        print(f'\\n  FAILED: {name} â€” {e}')\n",
    "        import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 7: CatBoost (Default + Tuned)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "cat_cols = list(X_train.select_dtypes(include='object').columns)\n",
    "X_train_cb = _catboost_fill_nan(X_train, cat_cols)\n",
    "X_test_cb = _catboost_fill_nan(X_test, cat_cols)\n",
    "X_full_cb = _catboost_fill_nan(X_full, cat_cols)\n",
    "\n",
    "# â”€â”€ CatBoost Default â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_catboost_default():\n",
    "    def train_fn(X, y):\n",
    "        X = _catboost_fill_nan(X, cat_cols)\n",
    "        kw = _catboost_kwargs()\n",
    "        kw['cat_features'] = cat_cols\n",
    "        kw['iterations'] = 2000\n",
    "        kw['early_stopping_rounds'] = 50   # stops early = faster + no overfitting\n",
    "        model = CatBoostClassifier(**kw)\n",
    "        from sklearn.model_selection import train_test_split as _tts\n",
    "        X_t, X_v, y_t, y_v = _tts(X, y, test_size=0.1, stratify=y, random_state=RANDOM_STATE)\n",
    "        model.fit(X_t, y_t, eval_set=(X_v, y_v), verbose=0)\n",
    "        return model\n",
    "    def predict_fn(model, X):\n",
    "        X = _catboost_fill_nan(X, cat_cols)\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "\n",
    "    _, metrics = measure_model('CatBoost (Default)', train_fn, predict_fn, X_train_cb, X_test_cb, y_train, y_test)\n",
    "\n",
    "    print('  CV AUC...')\n",
    "    metrics['auc_roc'] = cv_auc(train_fn, predict_fn, X_full_cb, y_full)\n",
    "    print(f'  CV AUC: {metrics[\"auc_roc\"]}')\n",
    "\n",
    "    print('  Scaling curve...')\n",
    "    scaling = scaling_curve(train_fn, predict_fn, X_train_cb, X_test_cb, y_train, y_test)\n",
    "\n",
    "    return {\n",
    "        'name': 'CatBoost (Default)', 'category': 'gradient_boosting', 'tuned': False,\n",
    "        'metrics': metrics, 'scaling': scaling,\n",
    "        'raw_data_handling': {'missing_values': 'native', 'categorical_features': 'native', 'class_imbalance': 'auto_class_weights'},\n",
    "    }\n",
    "\n",
    "# â”€â”€ CatBoost Tuned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_catboost_tuned():\n",
    "    def objective(trial):\n",
    "        kw = _catboost_kwargs()\n",
    "        kw['cat_features'] = cat_cols\n",
    "        kw['bootstrap_type'] = 'Bernoulli'   # required for subsample on GPU\n",
    "        kw.update({\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        })\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "        aucs = []\n",
    "        for tr_idx, val_idx in skf.split(X_train_cb, y_train):\n",
    "            model = CatBoostClassifier(**kw)\n",
    "            model.fit(X_train_cb.iloc[tr_idx], y_train.iloc[tr_idx],\n",
    "                      eval_set=(X_train_cb.iloc[val_idx], y_train.iloc[val_idx]),\n",
    "                      early_stopping_rounds=20, verbose=0)\n",
    "            pred = model.predict_proba(X_train_cb.iloc[val_idx])[:, 1]\n",
    "            aucs.append(roc_auc_score(y_train.iloc[val_idx], pred))\n",
    "        return np.mean(aucs)\n",
    "\n",
    "    search_start = time.time()\n",
    "    print(f'\\n  Optuna: {OPTUNA_TRIALS} trials...')\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=OPTUNA_TRIALS)\n",
    "    search_time = time.time() - search_start\n",
    "    best = study.best_params\n",
    "    kw = _catboost_kwargs()\n",
    "    kw['cat_features'] = cat_cols\n",
    "    kw['bootstrap_type'] = 'Bernoulli'\n",
    "    kw.update(best)\n",
    "\n",
    "    def train_fn(X, y):\n",
    "        model = CatBoostClassifier(**kw)\n",
    "        model.fit(X, y, verbose=0)\n",
    "        return model\n",
    "    def predict_fn(model, X):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "\n",
    "    _, metrics = measure_model('CatBoost (Tuned)', train_fn, predict_fn, X_train_cb, X_test_cb, y_train, y_test)\n",
    "    metrics['search_time_sec'] = round(search_time, 2)\n",
    "    metrics['fit_time_sec'] = metrics['train_time_sec']\n",
    "    metrics['train_time_sec'] = round(search_time + metrics['fit_time_sec'], 2)\n",
    "\n",
    "    print('  CV AUC...')\n",
    "    metrics['auc_roc'] = cv_auc(train_fn, predict_fn, X_full_cb, y_full)\n",
    "    print(f'  CV AUC: {metrics[\"auc_roc\"]}')\n",
    "\n",
    "    print('  Scaling curve...')\n",
    "    scaling = scaling_curve(train_fn, predict_fn, X_train_cb, X_test_cb, y_train, y_test)\n",
    "\n",
    "    return {\n",
    "        'name': 'CatBoost (Tuned)', 'category': 'gradient_boosting', 'tuned': True,\n",
    "        'metrics': metrics, 'scaling': scaling,\n",
    "        'raw_data_handling': {'missing_values': 'native', 'categorical_features': 'native', 'class_imbalance': 'auto_class_weights'},\n",
    "    }\n",
    "\n",
    "# â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for name, runner in [('CatBoost (Default)', run_catboost_default), ('CatBoost (Tuned)', run_catboost_tuned)]:\n",
    "    try:\n",
    "        results['models'].append(runner())\n",
    "        save_progress()\n",
    "    except Exception as e:\n",
    "        print(f'\\n  FAILED: {name} â€” {e}')\n",
    "        import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 8: AutoGluon\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import tempfile, shutil\n",
    "\n",
    "def run_autogluon():\n",
    "    ag_time = 600              # 10 min â€” 80GB GPU handles best_quality faster\n",
    "    ag_cv_time = 300            # 5 min per CV fold\n",
    "    ag_scale_time = 120         # 2 min per scaling point\n",
    "    ag_kw = {'num_gpus': 1} if USE_GPU else {}\n",
    "    tmpdir = tempfile.mkdtemp()\n",
    "\n",
    "    print('  Fitting AutoGluon...')\n",
    "    process = psutil.Process(os.getpid())\n",
    "    rss_before = process.memory_info().rss\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_df = X_train.copy()\n",
    "    train_df['TARGET'] = y_train.values\n",
    "    predictor = TabularPredictor(\n",
    "        label='TARGET', eval_metric='roc_auc', path=tmpdir, verbosity=1\n",
    "    ).fit(train_df, presets='best_quality', time_limit=ag_time, **ag_kw)\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    rss_after = process.memory_info().rss\n",
    "    peak_memory = max(0, rss_after - rss_before)\n",
    "\n",
    "    inf_start = time.time()\n",
    "    y_prob = predictor.predict_proba(X_test)[1].values\n",
    "    inf_time = time.time() - inf_start\n",
    "\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    ll = log_loss(y_test, y_prob)\n",
    "    inf_per_1k = (inf_time / len(X_test)) * 1000 * 1000\n",
    "\n",
    "    print(f'  AUC: {auc:.4f} | Log Loss: {ll:.4f}')\n",
    "    print(f'  Train: {train_time:.1f}s | Memory: {peak_memory / (1024*1024):.0f}MB')\n",
    "\n",
    "    metrics = {\n",
    "        'auc_roc': round(auc, 4), 'log_loss': round(ll, 4),\n",
    "        'train_time_sec': round(train_time, 2),\n",
    "        'inference_time_ms_per_1k': round(inf_per_1k, 1),\n",
    "        'peak_memory_mb': round(peak_memory / (1024 * 1024), 1),\n",
    "    }\n",
    "    shutil.rmtree(tmpdir, ignore_errors=True)\n",
    "\n",
    "    print('  CV AUC...')\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    aucs = []\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_full, y_full)):\n",
    "        fold_dir = tempfile.mkdtemp()\n",
    "        td = X_full.iloc[tr_idx].copy()\n",
    "        td['TARGET'] = y_full.iloc[tr_idx].values\n",
    "        pred_fold = TabularPredictor(\n",
    "            label='TARGET', eval_metric='roc_auc', path=fold_dir, verbosity=0\n",
    "        ).fit(td, presets='best_quality', time_limit=ag_cv_time, **ag_kw)\n",
    "        pred = pred_fold.predict_proba(X_full.iloc[val_idx])[1].values\n",
    "        fold_auc = roc_auc_score(y_full.iloc[val_idx], pred)\n",
    "        aucs.append(fold_auc)\n",
    "        print(f'    Fold {fold+1}: {fold_auc:.4f}')\n",
    "        shutil.rmtree(fold_dir, ignore_errors=True)\n",
    "    metrics['auc_roc'] = round(np.mean(aucs), 4)\n",
    "    print(f'  CV AUC: {metrics[\"auc_roc\"]}')\n",
    "\n",
    "    print('  Scaling curve...')\n",
    "    scaling = []\n",
    "    for n in SCALING_SIZES:\n",
    "        if n > len(X_train):\n",
    "            break\n",
    "        idx = X_train.sample(n=n, random_state=RANDOM_STATE).index\n",
    "        scale_dir = tempfile.mkdtemp()\n",
    "        td = X_train.loc[idx].copy()\n",
    "        td['TARGET'] = y_train.loc[idx].values\n",
    "        pred_scale = TabularPredictor(\n",
    "            label='TARGET', eval_metric='roc_auc', path=scale_dir, verbosity=0\n",
    "        ).fit(td, presets='best_quality', time_limit=ag_scale_time, **ag_kw)\n",
    "        pred = pred_scale.predict_proba(X_test)[1].values\n",
    "        sc_auc = roc_auc_score(y_test, pred)\n",
    "        scaling.append({'n_samples': n, 'auc': round(sc_auc, 4)})\n",
    "        print(f'    n={n}: AUC={sc_auc:.4f}')\n",
    "        shutil.rmtree(scale_dir, ignore_errors=True)\n",
    "\n",
    "    return {\n",
    "        'name': 'AutoGluon', 'category': 'automl', 'tuned': False,\n",
    "        'metrics': metrics, 'scaling': scaling,\n",
    "        'raw_data_handling': {'missing_values': 'native', 'categorical_features': 'native', 'class_imbalance': 'native'},\n",
    "    }\n",
    "\n",
    "try:\n",
    "    results['models'].append(run_autogluon())\n",
    "    save_progress()\n",
    "except Exception as e:\n",
    "    print(f'\\n  FAILED: AutoGluon â€” {e}')\n",
    "    import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 9: TabPFN\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "def run_tabpfn():\n",
    "    X_tr_enc, X_te_enc = encode_for_numeric(X_train, X_test)\n",
    "    X_full_enc = pd.concat([X_tr_enc, X_te_enc])  # for CV we re-encode per fold\n",
    "\n",
    "    if len(X_tr_enc) > TABPFN_MAX_SAMPLES:\n",
    "        print(f'  Subsampling to {TABPFN_MAX_SAMPLES} rows for TabPFN...')\n",
    "        idx = X_tr_enc.sample(n=TABPFN_MAX_SAMPLES, random_state=RANDOM_STATE).index\n",
    "        X_tr_sub = X_tr_enc.loc[idx]\n",
    "        y_tr_sub = y_train.loc[idx]\n",
    "    else:\n",
    "        X_tr_sub = X_tr_enc\n",
    "        y_tr_sub = y_train\n",
    "\n",
    "    def train_fn(X, y):\n",
    "        model = TabPFNClassifier(device=DEVICE, ignore_pretraining_limits=True)\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "    def predict_fn(model, X):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "\n",
    "    _, metrics = measure_model('TabPFN', train_fn, predict_fn, X_tr_sub, X_te_enc, y_tr_sub, y_test)\n",
    "\n",
    "    print('  CV AUC...')\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    aucs = []\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_full, y_full)):\n",
    "        X_tr_fold, X_val_fold = X_full.iloc[tr_idx], X_full.iloc[val_idx]\n",
    "        y_tr_fold, y_val_fold = y_full.iloc[tr_idx], y_full.iloc[val_idx]\n",
    "        X_tr_fold_enc, X_val_fold_enc = encode_for_numeric(X_tr_fold, X_val_fold)\n",
    "        if len(X_tr_fold_enc) > TABPFN_MAX_SAMPLES:\n",
    "            sub_idx = X_tr_fold_enc.sample(n=TABPFN_MAX_SAMPLES, random_state=RANDOM_STATE).index\n",
    "            X_tr_fold_enc = X_tr_fold_enc.loc[sub_idx]\n",
    "            y_tr_fold = y_tr_fold.loc[sub_idx]\n",
    "        model = train_fn(X_tr_fold_enc, y_tr_fold)\n",
    "        y_prob = predict_fn(model, X_val_fold_enc)\n",
    "        fold_auc = roc_auc_score(y_val_fold, y_prob)\n",
    "        aucs.append(fold_auc)\n",
    "        print(f'    Fold {fold+1}: {fold_auc:.4f}')\n",
    "    metrics['auc_roc'] = round(np.mean(aucs), 4)\n",
    "    print(f'  CV AUC: {metrics[\"auc_roc\"]}')\n",
    "\n",
    "    print('  Scaling curve...')\n",
    "    tabpfn_sizes = [s for s in SCALING_SIZES if s <= TABPFN_MAX_SAMPLES]\n",
    "    scaling = []\n",
    "    for n in tabpfn_sizes:\n",
    "        sub_idx = X_tr_enc.sample(n=n, random_state=RANDOM_STATE).index\n",
    "        model = train_fn(X_tr_enc.loc[sub_idx], y_train.loc[sub_idx])\n",
    "        y_prob = predict_fn(model, X_te_enc)\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        scaling.append({'n_samples': n, 'auc': round(auc, 4)})\n",
    "        print(f'    n={n}: AUC={auc:.4f}')\n",
    "\n",
    "    return {\n",
    "        'name': 'TabPFN', 'category': 'foundation_model', 'tuned': False,\n",
    "        'metrics': metrics, 'scaling': scaling,\n",
    "        'raw_data_handling': {'missing_values': 'native', 'categorical_features': 'needs_encoding', 'class_imbalance': 'none'},\n",
    "    }\n",
    "\n",
    "try:\n",
    "    results['models'].append(run_tabpfn())\n",
    "    save_progress()\n",
    "except Exception as e:\n",
    "    print(f'\\n  FAILED: TabPFN â€” {e}')\n",
    "    import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 10: FT-Transformer\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def run_ft_transformer():\n",
    "    X_tr_enc, X_te_enc = encode_for_numeric(X_train, X_test)\n",
    "\n",
    "    n_features = X_tr_enc.shape[1]\n",
    "    d_token = 192              # 3x wider tokens â€” 80GB can handle it\n",
    "    n_heads = 8                # more attention heads\n",
    "    n_layers = 6               # deeper model\n",
    "    d_ffn = 512                # 4x wider FFN\n",
    "    batch_size = 16384 if USE_GPU else 512   # 80GB VRAM â†’ huge batches\n",
    "    epochs = 50                # more epochs to converge with bigger model\n",
    "\n",
    "    if USE_GPU and torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    class FTTransformer(nn.Module):\n",
    "        def __init__(self, n_feat, d_tok, n_h, n_l, d_ff):\n",
    "            super().__init__()\n",
    "            self.feature_embeddings = nn.ModuleList([nn.Linear(1, d_tok) for _ in range(n_feat)])\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, d_tok))\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_tok, nhead=n_h, dim_feedforward=d_ff, dropout=0.1, batch_first=True)\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_l)\n",
    "            self.head = nn.Linear(d_tok, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            tokens = torch.stack([emb(x[:, i:i+1]) for i, emb in enumerate(self.feature_embeddings)], dim=1)\n",
    "            cls = self.cls_token.expand(x.size(0), -1, -1)\n",
    "            tokens = torch.cat([cls, tokens], dim=1)\n",
    "            return self.head(self.transformer(tokens)[:, 0]).squeeze(-1)\n",
    "\n",
    "    dev = torch.device(DEVICE)\n",
    "    use_amp = USE_GPU and torch.cuda.is_available()\n",
    "\n",
    "    def train_fn(X, y):\n",
    "        Xt = torch.tensor(X.values, dtype=torch.float32, device=dev)\n",
    "        yt = torch.tensor(y.values, dtype=torch.float32, device=dev)\n",
    "        dl = DataLoader(TensorDataset(Xt, yt), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model = FTTransformer(X.shape[1], d_token, n_heads, n_layers, d_ffn).to(dev)\n",
    "        if use_amp:\n",
    "            model = torch.compile(model, mode='reduce-overhead')\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for xb, yb in dl:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        loss = criterion(model(xb), yb)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss = criterion(model(xb), yb)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def predict_fn(model, X):\n",
    "        Xt = torch.tensor(X.values, dtype=torch.float32, device=dev)\n",
    "        with torch.no_grad():\n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                    logits = model(Xt)\n",
    "            else:\n",
    "                logits = model(Xt)\n",
    "            return torch.sigmoid(logits.float()).cpu().numpy()\n",
    "\n",
    "    _, metrics = measure_model('FT-Transformer', train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "\n",
    "    print('  CV AUC...')\n",
    "    def prep(Xtr, Xte):\n",
    "        return encode_for_numeric(Xtr, Xte)\n",
    "    metrics['auc_roc'] = cv_auc(train_fn, predict_fn, X_full, y_full, prep_fn=prep)\n",
    "    print(f'  CV AUC: {metrics[\"auc_roc\"]}')\n",
    "\n",
    "    print('  Scaling curve...')\n",
    "    scaling = scaling_curve(train_fn, predict_fn, X_tr_enc, X_te_enc, y_train, y_test)\n",
    "\n",
    "    if USE_GPU:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        'name': 'FT-Transformer', 'category': 'deep_learning', 'tuned': False,\n",
    "        'metrics': metrics, 'scaling': scaling,\n",
    "        'raw_data_handling': {'missing_values': 'needs_imputation', 'categorical_features': 'needs_encoding', 'class_imbalance': 'none'},\n",
    "    }\n",
    "\n",
    "try:\n",
    "    results['models'].append(run_ft_transformer())\n",
    "    save_progress()\n",
    "except Exception as e:\n",
    "    print(f'\\n  FAILED: FT-Transformer â€” {e}')\n",
    "    import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cell 11: Summary + Download results\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from google.colab import files\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f'  BENCHMARK COMPLETE')\n",
    "print(f'  {len(results[\"models\"])} models benchmarked')\n",
    "print(f\"{'='*60}\")\n",
    "print(f'  Hardware: {results[\"hardware\"].get(\"gpu\", results[\"hardware\"][\"cpu\"])}')\n",
    "print(f'  Dataset: {results[\"n_samples\"]} samples, {results[\"n_features\"]} features')\n",
    "print(f'  Target rate: {results[\"target_rate\"]}\\n')\n",
    "\n",
    "for m in sorted(results['models'], key=lambda x: -x['metrics']['auc_roc']):\n",
    "    print(f\"  {m['name']:<25s}  AUC: {m['metrics']['auc_roc']:.4f}  Time: {m['metrics']['train_time_sec']:.1f}s\")\n",
    "\n",
    "# Save final\n",
    "with open(OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f'\\nğŸ“¥ Downloading credit_results.json...')\n",
    "print(f'   Save it to: tabular-arena/results/credit_results.json')\n",
    "files.download(str(OUTPUT_PATH))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": { "gpuType": "A100", "provenance": [] },
  "kernelspec": { "display_name": "Python 3", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
